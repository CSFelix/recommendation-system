{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db7a2b2-5dbf-40f6-96b4-fb135e7c0bbb",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1 id='content-based-filtering' style='color:#7159c1; font-size:350%'>Collaborative Filtering</h1>\n",
    "    <i style='font-size:125%'>Recommendations of Items from Similar Users</i>\n",
    "</center>\n",
    "\n",
    "> **Topics**\n",
    "\n",
    "```\n",
    "- ✨ Content-Based Filtering Problems\n",
    "- ✨ Collaborative Filtering\n",
    "- ✨ User Based Approach\n",
    "- ✨ K-Nearest Neighbors\n",
    "- ✨ Hands-on\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f08f2-8ac2-42d6-bcd5-bfd329df2c97",
   "metadata": {},
   "source": [
    "<h1 id='0-content-based-filtering-problems' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>✨ | Content-Based Filtering Problems</h1>\n",
    "\n",
    "In the previous two notebooks, we dived into Content-Based Filtering with Plot Description and Metadatas approach and got better recommendations results!!\n",
    "\n",
    "Nonetheless, you may be wondering: *\"Okay, where is the catch? Is this method really perfect? Are there any problems with it?\"*. And yes, even though giving better results, there are some cons on Content-Based Filtering.\n",
    "\n",
    "The first problem is that the recommendations are based on similiar items regardless the user tastes. Picture this, if user A and user B are into Mob Psycho 100, they both will receive the same similar animes recommendations, regardless their animes tastes and, consequently, a Recommendation Bubble is created.\n",
    "\n",
    "Besides, people tastes change over the time, so, even though user A are into shounen animes like Mob Psycho 100 today, in a few weeks this very user can be into slice-of-life animes and, since the given recommendations will be using Mob Psycho 100 as a parameter, the user will not receive any slice-of-life animes recommendations, leading to the user search for another platform to watch what he is looking for.\n",
    "\n",
    "Thus, in order to minimize these problems, a new recommendation method has been made up: `Collaborative Filtering`!! Let's find out what it is and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13085e0e-900f-4ed6-a50c-fdc45b33d8d1",
   "metadata": {},
   "source": [
    "<h1 id='1-collaborative-filtering' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>✨ | Collaborative Filtering</h1>\n",
    "\n",
    "`Collaborative Filtering` reccomends animes that similar users liked, being able to get closer to the real users tastes. If you use Netflix, you probably already stumbled upon to some series marked as `For you`. If that's so, congrats, that is a real-world Collaborative Filtering Recommendation!! To make things even clearer, assume that two similar users, user A and user B, like Demon Slayer, and user B is also into Grand Blue, so the platform will recommend Grand Blue to user A.\n",
    "\n",
    "Besides, this Filtering has two modes: 1) `User-Based`, where the user receives recommendations from items that similar users liked; and 2) `Item-Based`, where the user receives recommendations from items that similar users liked and the current user may well rate the recommended item.\n",
    "\n",
    "\n",
    "About the advantages:\n",
    "\n",
    "> **Better Recommendations** - `since it recommends animes that similar users liked, this system method tends to get closer to the user tastes when compared to Content-Based and Demographic Filtering`;\n",
    "\n",
    "> **Personalized Recommendations** - `even though two users are searching for recommendations using the same anime, for instance Mob Psycho 100, both of them will receive different recommendations due to their tastes`;\n",
    "\n",
    "> **Low Bubble of Recommendations** - `consequently, the probability of a Bubble of Recommendations be created is low and, even if one is created, it will be small`.\n",
    "\n",
    "<br />\n",
    "\n",
    "Disadvantages-wise:\n",
    "\n",
    "> **More Data Required** - `ir order to get closer to users tastes, in addition to having animes data, it is needed to have users data, such as their ratings on previously watched animes`;\n",
    "\n",
    "> **Bubble of Recommendations** - `even though the probability of a small Bubble of Recommendations be created is low, there is yet the risk of it be happening`;\n",
    "\n",
    "> **Outliers** - `it is needed to add a cut-off of users ratings and mean rating score by user in order to avoid outliers in the recommendations. For instance, consider that user A rated 100 animes with 1 score and the very user mean score of all rated animes is 1.5, it means that the user bad rated all animes he watched and, consequently, may be up no good in the platform giving outliers to the ratings`;\n",
    "\n",
    "> **More Computational Cost and Power** - `Collaborative-Filtering Algorithms are more complex and sofisticated to the previously ones, then, more computational cost and power is needed to run them`.\n",
    "\n",
    "<br />\n",
    "\n",
    "The image below ilustrates how this technique works:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/2-collaborative-filtering.png' alt='Collaborative Filtering Diagram' />\n",
    "    <figcaption>Figure 1 - Collaborative Filtering Diagram. By <a href='https://www.analyticsvidhya.com/blog/2022/02/introduction-to-collaborative-filtering/'>Shivam Baldha - Introduction to Collaborative Filtering©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "In this notebook, we are going further to User-Based technique and use K-Nearest Neighbors to find similar users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca383d-135b-425a-80f6-34d0d32c5e64",
   "metadata": {},
   "source": [
    "<h1 id='2-user-based-approach' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>✨ | User-Based Approach</h1>\n",
    "\n",
    "In a few words, consider a person called user E, the `Collaborative Filtering User-Based Approach` works on finding similar users to user E and then recommendating to him similar items that the similar users liked.\n",
    "\n",
    "To do it, the Algorithm first calculates the similarity between the users using `Pearson Correlation, Cosine Similarity or other metric`, then predicts the rate user E would give to the animes that the most similar users have watched and recommends the most predicted, rated ones.\n",
    "\n",
    "For example, consider the following situation where want to recommend movies to user E. The first step is to calculate the similarity of the others users to this one. The image below ilustrates the situation:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/18.0-collaborative-filtering-user-based.png' alt='Collaborative Filtering example using User-Based approach' />\n",
    "    <figcaption>Figure 2 - Board ilustrating the similarity of the users to user E. The indexes are the users, the columns are the movies and the users rates to the movies and the last column is the similarity of the users to user E. The similarity has been calculated using Pearson Correlation. Besides, since user A and F have not rated movies that user E has been, their similarity is 0 (NaN). Since the similarity is being calculated to user E, user E has full similarity to itself; also, user D is totally different to user E due to the similarity be -1. By <a href='https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system'>Sibtesam Ahmed - Getting Started with a Movie Recommendation System©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "After calculating the similarities, we have to predict the ratings that user E would give to the movies he hasn't rated and then, recommends to him the movies liked by the most similar users and that got the higher predicted ratings from user E. The following image pictures the results:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/18.1-collaborative-filtering-user-based.png' alt='Collaborative Filtering example results using User-Based approach' />\n",
    "    <figcaption>Figure 3 -Board ilustrating the results of the Collaborative Filtering. The predicted ratings of user E are marked with asterisks (*). The most similar users to user E are C and B. Probably, Avengers would be the recommended movie since its the movie that a similar user (B) has liked and got a high predicted rating to user E. By <a href='https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system'>Sibtesam Ahmed - Getting Started with a Movie Recommendation System©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0805a-f6a8-4e8f-b5f6-bd40f7b137d1",
   "metadata": {},
   "source": [
    "<h1 id='3-k-nearest-neighbors' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>✨ | K-Nearest Neighbors</h1>\n",
    "\n",
    "Instead of be using Pearson Correlation or Cosine Similarity to find similar users to a given one, we are going further and to apply `K-Nearest Neighbors` to do this task. This algorithm does one thing different than what has been done in the example from the previous section: instead of finding similar users, it consider that the users are grouped into clusters and its major goal is to find the most similar cluster to a given user.\n",
    "\n",
    "K-Nearest Neighbores works like this:\n",
    "\n",
    "> 1 - group the users into clusters (when the categories are known, we can stick into them. When the categories are unknown, we can use Unsupervisioned Machine Learning Algorithms, such as `K-Means Clustering`, to cluster the data);\n",
    "\n",
    "> 2 - for a given user, find the K nearest neighbors, being \"K\" the number of nearest neighbors to be considered;\n",
    "\n",
    "> 3 - when \"K\" is equals to 1, the given user is similar to the cluster of the unique nearest neighbor. When \"K\" is greater than 1, the given user is similar to the cluster of the most nearest neighbors belong. If there are a tie, we randomly choose one of the tied clusters to the given user be similar (picture that the user E has 5 nearest neighbors from cluster Red and 5 others from cluster Blue. Since both clusters has the same amount of users chosen as nearest neighbors to the given user, we randomly choose between Red and Blue to the very user be similar to).\n",
    "\n",
    "<br />\n",
    "\n",
    "The image below pictures an example of the clustering:\n",
    "\n",
    "<br />\n",
    "\n",
    "<figure style='text-align:center'>\n",
    "    <img style='border-radius:20px' src='./assets/19-k-nearest-neighbors.png' alt='Example of K-Nearest Neighbors Algorithm assigning a cluster to a given data point' />\n",
    "    <figcaption>Figure 4 - Example of K-Nearest Neighbors Algorithm assigning a cluster to a given data point. By <a href='https://www.youtube.com/watch?v=HVXime0nQeI'>StatQuest with Josh Starmer - StatQuest: K-nearest neighbors, Clearly Explained©</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "About the value of Nearest Neighbors (K) to be taken, we have to consider these information:\n",
    "\n",
    "> 1 - There is no phisical or biological way to determine the best value for \"K\", so you may have to try a few out values  before settling on one. Do this by pretending part of the training data is \"unknown\";\n",
    "\n",
    "> 2 - Low values for K, such as K=1 or K=2, can be noisy and subject to the effects of outliers;\n",
    "\n",
    "> 3 - Large values for K smooth over things, but you do not want to K be so large that a category with only a few samples in it will always be out voted by other categories.\n",
    "\n",
    "<br />\n",
    "\n",
    "For better explanations about how K-Nearest Neighbors and K-Means Clustering work, consider watching these two videos provided by [StatQuest with Josh Starmer](https://www.youtube.com/@statquest): [StatQuest: K-nearest neighbors, Clearly Explained](https://www.youtube.com/watch?v=HVXime0nQeI) and [StatQuest: K-means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728b40f-30c2-41c4-b7b8-d277dd5cc02a",
   "metadata": {},
   "source": [
    "<h1 id='4-hands-on' style='color:#7159c1; border-bottom:3px solid #7159c1; letter-spacing:2px; font-family:JetBrains Mono; font-weight: bold; text-align:left; font-size:240%;padding:0'>✨ | Hands-on</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19086d0b-8749-463e-85b9-1908d7556897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
